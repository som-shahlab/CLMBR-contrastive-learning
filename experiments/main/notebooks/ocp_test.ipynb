{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ehr_ml.clmbr import PatientTimelineDataset\n",
    "from ehr_ml.clmbr.dataset import DataLoader\n",
    "from ehr_ml.timeline import TimelineReader\n",
    "from ehr_ml.clmbr import convert_patient_data\n",
    "import google.auth\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch/nigam/envs/jlemmon/conl/lib/python3.9/site-packages/google/auth/_default.py:79: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "rs_dataset_project = 'som-nero-nigam-starr'\n",
    "rs_dataset = 'jlemmon_explore'\n",
    "rs_table = 'clmbr_admission_rollup'\n",
    "clmbr_model_path = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/artifacts/models/clmbr/pretrained/models/gru_sz_800_do_0.1_cd_0_dd_0_lr_0.001_l2_0.01'\n",
    "cohort_fpath = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/'\n",
    "extract_path = os.path.join('/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/extracts/20210723', \"extract.db\")\n",
    "labelled_fpath = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/labelled_data/hospital_mortality/pretrained/gru_sz_800_do_0.1_cd_0_dd_0_lr_0.001_l2_0.01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'SELECT * FROM {rs_dataset_project}.{rs_dataset}.{rs_table}'\n",
    "\n",
    "adm_df = pd.read_gbq(query, project_id=rs_dataset_project, dialect='standard', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        person_id          admit_date      discharge_date\n",
      "0        29923082 2018-10-16 13:30:00 2018-10-24 13:08:00\n",
      "1        29923083 2018-07-22 20:22:00 2018-07-26 14:45:00\n",
      "2        29923090 2018-02-04 08:05:00 2018-02-09 11:27:00\n",
      "3        29923110 2015-06-27 17:03:00 2015-06-30 12:20:00\n",
      "4        29923110 2020-10-27 09:06:00 2020-10-31 14:55:00\n",
      "...           ...                 ...                 ...\n",
      "472145   69241533 2021-06-23 18:10:00 2021-06-25 15:51:00\n",
      "472146   69241534 2021-06-16 11:30:00 2021-06-19 12:23:00\n",
      "472147   69241537 2021-07-02 16:57:00 2021-07-04 14:31:00\n",
      "472148   69241546 2021-08-04 05:52:00 2021-08-06 13:09:00\n",
      "472149   69241548 2021-08-08 12:57:00 2021-08-09 11:28:00\n",
      "\n",
      "[472150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(adm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    person_id          admit_date      discharge_date\n",
      "3    29923110 2015-06-27 17:03:00 2015-06-30 12:20:00\n",
      "4    29923110 2020-10-27 09:06:00 2020-10-31 14:55:00\n",
      "5    29923118 2019-01-07 10:36:00 2019-01-22 17:45:00\n",
      "6    29923118 2019-02-08 23:17:00 2019-02-17 15:46:00\n",
      "8    29923122 2019-06-17 17:38:00 2019-06-17 23:59:00\n",
      "9    29923122 2019-06-19 06:15:00 2019-06-23 12:30:00\n",
      "10   29923142 2019-01-13 17:38:00 2019-01-16 16:08:00\n",
      "11   29923142 2019-02-23 06:46:00 2019-02-23 08:36:00\n",
      "20   29923194 2018-11-20 09:31:00 2018-11-22 17:42:00\n",
      "21   29923194 2020-09-02 14:11:00 2020-09-03 17:50:00\n"
     ]
    }
   ],
   "source": [
    "filter_df = adm_df[adm_df.groupby('person_id').person_id.transform('count') >1][:10]\n",
    "print(filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pids = list(filter_df['person_id'].unique())\n",
    "ehr_val_pids, ehr_val_days = convert_patient_data(extract_path, filter_df['person_id'], filter_df['admit_date'])\n",
    "print(ehr_val_pids, ehr_val_days)\n",
    "#print(val_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_ids = pd.DataFrame({'pid':list(filter_df['person_id']),'ehr_id':ehr_val_pids, 'admit_date':list(filter_df['admit_date']), 'discharge_date':list(filter_df['discharge_date']),'day_idx':ehr_val_days})\n",
    "print(ocp_ids)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load datasets from split csv files.\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = f'{labelled_fpath}'\n",
    "\n",
    "\n",
    "    train_pids = pd.read_csv(f'{data_path}/ehr_ml_patient_ids_train.csv')\n",
    "    # \tval_pids = pd.read_csv(f'{data_path}/ehr_ml_patient_ids_val.csv')\n",
    "\n",
    "    train_days = pd.read_csv(f'{data_path}/day_indices_train.csv')\n",
    "    # \tval_days = pd.read_csv(f'{data_path}/day_indices_val.csv')\n",
    "\n",
    "    train_labels = pd.read_csv(f'{data_path}/labels_train.csv')\n",
    "    # \tval_labels = pd.read_csv(f'{data_path}/labels_val.csv')\n",
    "\n",
    "    train_data = (train_labels.to_numpy().flatten(),train_pids.to_numpy().flatten(),train_days.to_numpy().flatten())\n",
    "    # \tval_data = (val_labels.to_numpy().flatten(),val_pids.to_numpy().flatten(),val_days.to_numpy().flatten())\n",
    "    df = pd.DataFrame({'pids':train_data[1], 'days':train_data[2]})\n",
    "\n",
    "    return df, train_pids\n",
    "    \n",
    "def get_windows(adm_df, pid):\n",
    "    admissions = adm_df.query('person_id == @pid')\n",
    "    admissions.reset_index(inplace=True)\n",
    "    num_windows = int(len(admissions)/2)\n",
    "    window_pairs = []\n",
    "    idx = 0\n",
    "    for i in range(num_windows):\n",
    "        w_1 = admissions.iloc[idx]\n",
    "        w_2 = admissions.iloc[idx+1]\n",
    "        window_pairs.append([(w_1['person_id'],w_1['admit_date'],w_1['discharge_date']),(w_2['person_id'],w_2['admit_date'],w_2['discharge_date'])])\n",
    "        print('dsaasdsa')\n",
    "        print(window_pairs)\n",
    "        print('dsasadas')\n",
    "        \n",
    "        idx+=2\n",
    "    return window_pairs\n",
    "\n",
    "def get_batch(window_pair):\n",
    "    tlr = TimelineReader(extract_path)\n",
    "    \n",
    "    batch = []\n",
    "    for wp in window_pair:\n",
    "        print(wp[0])\n",
    "        print(wp[1])\n",
    "        p1 = tlr.get_patient(wp[0][0],wp[0][1],wp[0][2])\n",
    "        p2 = tlr.get_patient(wp[1][0],wp[1][1],wp[1][2])\n",
    "        rint = np.random.randint(1,2)\n",
    "\n",
    "        print(wp[0][2]-wp[0][1])\n",
    "        \n",
    "#         if rint == 1:\n",
    "#             days = list(p1.days) + list(p2.days)\n",
    "#         else:\n",
    "#             days = list(p2.days) + list(p1.days)\n",
    "#         print(days)\n",
    "    \n",
    "    return batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_df, t_pids = load_data()\n",
    "print(pid_df)\n",
    "# print(val_pids)\n",
    "for pid in val_pids:\n",
    "    print(pid)\n",
    "    if pid in t_pids:\n",
    "        print(pid)\n",
    "pid_df = pid_df[pid_df['pids'].isin(val_pids)]\n",
    "print(pid_df)\n",
    "pid = pid_df['pids'][0]\n",
    "day_idx = pid_df['days'][0]\n",
    "train_data = (np.array(pid), np.array([0]), np.array(day_idx))\n",
    "print(train_data)\n",
    "train_dataset = PatientTimelineDataset(extract_path + '/extract.db', \n",
    "                                         extract_path + '/ontology.db', \n",
    "                                         f'{clmbr_model_path}/info.json', \n",
    "                                         train_data, \n",
    "                                         train_data )\n",
    "# pid = np.random.choice(pids,size=1)[0]\n",
    "# wp = get_windows(filter_df,pid)\n",
    "# batch = get_batch(wp)\n",
    "# with Dataloader(batch, model.config['num_first'], is_val=False, batch_size=9999999, device=args.device) as train_loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 1, 1, 1],\n",
      "         [2, 2, 2, 2, 2, 2],\n",
      "         [3, 3, 3, 3, 3, 3]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0]]])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pad_sequence(): argument 'sequences' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pairs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(pairs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m packed_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(packed_sequence)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(packed_sequence[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/local-scratch/nigam/envs/jlemmon/conl/lib/python3.9/site-packages/torch/nn/utils/rnn.py:398\u001b[0m, in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Packs a list of variable length Tensors\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m``sequences`` should be a list of Tensors of size ``L x *``, where `L` is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    a :class:`PackedSequence` object\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([v\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sequences])\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_padded_sequence(\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m, lengths, enforce_sorted\u001b[38;5;241m=\u001b[39menforce_sorted)\n",
      "File \u001b[0;32m/local-scratch/nigam/envs/jlemmon/conl/lib/python3.9/site-packages/torch/nn/utils/rnn.py:363\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Pad a list of variable length Tensors with ``padding_value``\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m``pad_sequence`` stacks a list of Tensors along a new dimension,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Tensor of size ``B x T x *`` otherwise\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: pad_sequence(): argument 'sequences' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "pairs = [torch.tensor([[1,1,1,1,1,1],[2,2,2,2,2,2],[3,3,3,3,3,3]]),torch.tensor([[1,1,1,1,1,1]])]\n",
    "pairs = nn.utils.rnn.pad_sequence(pairs, batch_first=True)\n",
    "print(pairs)\n",
    "print(pairs.shape)\n",
    "packed_sequence = nn.utils.rnn.pack_sequence(pairs, enforce_sorted=False)\n",
    "print(packed_sequence)\n",
    "print(packed_sequence[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conl",
   "language": "python",
   "name": "conl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Cohort, get train/val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cohort_dir = \"/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/cohort\"\n",
    "\n",
    "# load cohort\n",
    "df_cohort = pd.read_parquet(\n",
    "    os.path.join(\n",
    "        cohort_dir,\n",
    "        \"cohort_split.parquet\",\n",
    "    ),\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# datetime -> date\n",
    "df_cohort = df_cohort.assign(date = pd.to_datetime(df_cohort['admit_date']).dt.date)\n",
    "\n",
    "# get train/val sets\n",
    "train = df_cohort.query(\n",
    "    f\"hospital_mortality_fold_id!=['val','test','ignore'] and admission_year==[2008,2009,2010,2011,2012,2013,2014,2015,2016]\"\n",
    ")\n",
    "    \n",
    "val = df_cohort.query(\n",
    "    f\"hospital_mortality_fold_id==['val'] and admission_year==[2008,2009,2010,2011,2012,2013,2014,2015,2016]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert and save person IDs for CLMBR info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/labelled_data/hospital_mortality/pretrained/gru_sz_800_do_0.1_cd_0_dd_0_lr_0.001_l2_0.01'\n",
    "extracts_dir = \"/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/data/extracts/20210723\"\n",
    "clmbr_model_path = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/artifacts/models/clmbr/pretrained/models/gru_sz_800_do_0.1_cd_0_dd_0_lr_0.001_l2_0.01'\n",
    "model_debug_path = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/logging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lists of patient info for hospital_mortality task\n",
    "train_pids = pd.read_csv(f'{data_path}/ehr_ml_patient_ids_train.csv').to_numpy().flatten()\n",
    "val_pids = pd.read_csv(f'{data_path}/ehr_ml_patient_ids_val.csv').to_numpy().flatten()\n",
    "\n",
    "train_days = pd.read_csv(f'{data_path}/day_indices_train.csv').to_numpy().flatten()\n",
    "val_days = pd.read_csv(f'{data_path}/day_indices_val.csv').to_numpy().flatten()\n",
    "\n",
    "train_labels = pd.read_csv(f'{data_path}/labels_train.csv').to_numpy().flatten()\n",
    "val_labels = pd.read_csv(f'{data_path}/labels_val.csv').to_numpy().flatten()\n",
    "\n",
    "train_data = (train_labels,train_pids,train_days)\n",
    "val_data = (val_labels,val_pids,val_days)\n",
    "\n",
    "# before creating dataset length of pids matches length of labels\n",
    "assert(len(train_labels) == len(train_pids)==len(train_days))\n",
    "assert(len(val_labels) == len(val_pids)==len(val_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Patient Timeline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ehr_ml.clmbr import PatientTimelineDataset\n",
    "\n",
    "# generate dataset\n",
    "dataset = PatientTimelineDataset(\n",
    "    os.path.join(extracts_dir, \"extract.db\"), \n",
    "    os.path.join(extracts_dir, \"ontology.db\"),\n",
    "    os.path.join(clmbr_model_path, \"info.json\"),\n",
    "    train_data,\n",
    "    val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-to-end Binary Classification Model with CLMBR as Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ehr_ml.clmbr.rnn_model import PatientRNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BinaryLinearCLMBRClassifier(nn.Module):\n",
    "            \n",
    "    def __init__(self, model, device=None):\n",
    "        super().__init__()\n",
    "        self.config = model.config\n",
    "        self.timeline_model = model.timeline_model\n",
    "        self.linear = nn.Linear(model.config[\"size\"], 1)\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = nn.Sigmoid()\n",
    "        self=self.to(self.device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = dict()\n",
    "        \n",
    "        embedding = self.timeline_model(batch[\"rnn\"])\n",
    "\n",
    "        label_indices, label_values = batch[\"label\"]\n",
    "\n",
    "        flat_embeddings = embedding.view((-1, embedding.shape[-1]))\n",
    "        \n",
    "        target_embeddings = F.embedding(label_indices, flat_embeddings) \n",
    "        \n",
    "        logits = self.linear(target_embeddings).flatten()\n",
    "        \n",
    "        outputs['pids']=batch['pid']\n",
    "        outputs['pred_probs'] = self.criterion(logits)\n",
    "        outputs['labels'] = label_values\n",
    "        outputs['loss'] = F.binary_cross_entropy_with_logits(\n",
    "            logits, label_values.float(), reduction=\"sum\"\n",
    "        )\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        pred_probs = []\n",
    "        labels = []\n",
    "        pids = []\n",
    "        mismatch_pids = []\n",
    "        pbar = tqdm(total=dataloader.num_batches)\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if len(batch['pid']) != len(batch['label']):\n",
    "                    mismatch_pids.extend(batch['pid'])\n",
    "                outputs = self.forward(batch)\n",
    "                pred_probs.extend(list(outputs['pred_probs'].cpu().numpy()))\n",
    "                labels.extend(outputs['labels'].cpu().numpy())\n",
    "                pids.extend(outputs['pids'])\n",
    "                pbar.update(1)\n",
    "                \n",
    "        return {\n",
    "            'pid': pids,\n",
    "            'labels': labels,\n",
    "            'pred_probs': pred_probs,\n",
    "            'mismatch_pids': mismatch_pids,\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def load_weights(self,model_dir):\n",
    "        \n",
    "        model_data = torch.load(\n",
    "            os.path.join(model_dir,\"best\")\n",
    "        )\n",
    "        \n",
    "        self.load_state_dict(model_data)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Model Config and Load CLMBR info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ehr_ml.clmbr.utils import read_info\n",
    "import ehr_ml\n",
    "import json\n",
    "\n",
    "model_dir = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/artifacts/models/clmbr/pretrained/models/gru_sz_800_do_0.1_cd_0_dd_0_lr_0.001_l2_0.01'\n",
    "\n",
    "clmbr_model = ehr_ml.clmbr.CLMBR.from_pretrained(clmbr_model_path, 'cuda:0').to('cuda:0')\n",
    "clmbr_model.config['model_dir'] = '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/model'\n",
    "clmbr_model.config['epochs_per_cycle'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = BinaryLinearCLMBRClassifier(clmbr_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:52:12,007 Args: {'batch_size': 2000, 'eval_batch_size': 1, 'num_first': 9262, 'num_second': 10044, 'size': 800, 'lr': 0.001, 'dropout': 0.1, 'encoder_type': 'gru', 'rnn_layers': 1, 'tied_weights': True, 'l2': 0.01, 'b1': 0.9, 'b2': 0.999, 'e': 1e-08, 'epochs_per_cycle': 5, 'warmup_epochs': 2, 'code_dropout': 0.0, 'day_dropout': 0.0, 'model_dir': '/local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/model'}\n",
      "2022-04-04 18:52:12,010 Batches per epoch = 1124\n",
      "2022-04-04 18:52:12,010 Total batches = 5620\n",
      "2022-04-04 18:52:12,011 Start training\n",
      "2022-04-04 18:52:12,012 About to start epoch 0\n",
      "/local-scratch/nigam/envs/jlemmon/conl/lib/python3.9/site-packages/ehr_ml/clmbr/opt.py:115: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "2022-04-04 18:52:12,317 Seen batch 0\n",
      "2022-04-04 18:53:33,103 Epoch 0 is complete\n",
      "2022-04-04 18:55:36,847 Train loss: 0.04936415276911066\n",
      "2022-04-04 18:55:36,850 Val loss: 0.07262082224487312\n",
      "2022-04-04 18:55:36,974 Saving best model to /local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/model/best\n",
      "2022-04-04 18:55:36,975 About to start epoch 1\n",
      "2022-04-04 18:55:37,216 Seen batch 0\n",
      "2022-04-04 18:56:58,116 Epoch 1 is complete\n",
      "2022-04-04 18:59:01,646 Train loss: 0.03250603162213133\n",
      "2022-04-04 18:59:01,648 Val loss: 0.05534594477713881\n",
      "2022-04-04 18:59:01,766 Saving best model to /local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/model/best\n",
      "2022-04-04 18:59:01,767 About to start epoch 2\n",
      "2022-04-04 18:59:02,076 Seen batch 0\n",
      "2022-04-04 19:00:22,584 Epoch 2 is complete\n",
      "2022-04-04 19:02:26,352 Train loss: 0.023237864836391283\n",
      "2022-04-04 19:02:26,354 Val loss: 0.05430238906971192\n",
      "2022-04-04 19:02:26,469 Saving best model to /local-scratch/nigam/projects/jlemmon/cl-clmbr/experiments/main/debug/model/best\n",
      "2022-04-04 19:02:26,470 About to start epoch 3\n",
      "2022-04-04 19:02:26,776 Seen batch 0\n",
      "2022-04-04 19:03:47,679 Epoch 3 is complete\n",
      "2022-04-04 19:05:51,234 Train loss: 0.012722788167116451\n",
      "2022-04-04 19:05:51,237 Val loss: 0.06977517628553219\n",
      "2022-04-04 19:05:51,238 About to start epoch 4\n",
      "2022-04-04 19:05:51,520 Seen batch 0\n",
      "2022-04-04 19:07:12,584 Epoch 4 is complete\n",
      "2022-04-04 19:09:16,282 Train loss: 0.008774867844299867\n",
      "2022-04-04 19:09:16,284 Val loss: 0.07882599185133261\n",
      "2022-04-04 19:09:16,285 Training complete!\n"
     ]
    }
   ],
   "source": [
    " # have trained model saved, can skip this cell\n",
    "from ehr_ml.clmbr import Trainer\n",
    "from ehr_ml.utils import set_up_logging\n",
    "import logging\n",
    "\n",
    "set_up_logging(os.path.join(model_debug_path,'train.log'))\n",
    "logging.info(\"Args: %s\", str(model.config))\n",
    "trainer = Trainer(model)\n",
    "trainer.train(dataset, use_pbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_weights(model.config['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct dataloader for validation set to get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ehr_ml.clmbr.dataset import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, threshold = model.config['num_first'], is_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:08, 26.97it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(dataloader)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_probs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs = model.predict(dataloader)\n",
    "assert(len(outputs['pid'])==len(outputs['labels'])==len(outputs['pred_probs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18867, 18836, 18836)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different lengths of pids vs. labels & pred_probs\n",
    "# seems to only happen to 31 pids\n",
    "len(outputs['pid']), len(outputs['labels']), len(outputs['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date for day ID: 2011-03-14 patient's admission date: 2011-03-14 23:00:00\n",
      "date for day ID: 2016-03-13 patient's admission date: 2016-03-13 16:42:00\n",
      "date for day ID: 2016-08-03 patient's admission date: 2016-08-03 07:52:00\n",
      "date for day ID: 2014-11-24 patient's admission date: 2014-11-24 13:38:00\n",
      "date for day ID: 2016-07-28 patient's admission date: 2016-07-28 21:24:00\n",
      "date for day ID: 2013-04-04 patient's admission date: 2013-04-04 23:52:00\n",
      "date for day ID: 2008-01-22 patient's admission date: 2008-01-22 12:20:00\n"
     ]
    }
   ],
   "source": [
    "from ehr_ml import timeline\n",
    "\n",
    "timelines = timeline.TimelineReader(os.path.join(extracts_dir, \"extract.db\"))\n",
    "\n",
    "# In my main script I've tried printing the pid list for a batch if a mismatch was found\n",
    "# example pids from last batch of validation, one of these pids has no prediction/label\n",
    "# can't tell which one is the problem pid, all the pids seem to be within my validation range (2008-01-01 to 2016-12-31)\n",
    "# not sure why the dataloader/dataset is not loading the timeline/labels for these pids\n",
    "for pid in [2574937, 2163575, 1216845, 162573, 1985692, 1389474, 1023077]:\n",
    "    patient = timelines.get_patient(pid)\n",
    "    print(\n",
    "        \"date for day ID:\",\n",
    "        patient.days[\n",
    "            val_days[\n",
    "                np.where(val_pids==pid)[0][0]\n",
    "            ]\n",
    "        ].date,\n",
    "        \"patient's admission date:\", \n",
    "        val.iloc[np.where(val_pids==pid)[0][0]]['admit_date']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9725277259793288"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as auc\n",
    "auc(outputs['labels'], outputs['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conl",
   "language": "python",
   "name": "conl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
